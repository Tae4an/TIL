# 파인튜닝(Fine-tuning)이란?

## 개념 정의
파인튜닝(Fine-tuning, 미세 조정)은 **이미 대규모 데이터로 사전 학습된 머신러닝/딥러닝 모델을 특정 작업(Task)이나 도메인에 맞게 추가로 학습시켜 성능을 최적화하는 전이학습(Transfer Learning) 기법**이다.

쉽게 말해 "범용적으로 학습된 AI 모델을, 내가 원하는 특정 분야·작업에 더 잘 맞도록 가중치를 조금씩 조정하는 과정"이다.

## 핵심 원리

**사전 학습 모델 활용**
- GPT, BERT, ResNet 등 대규모 데이터(수십억 단어, 수백만 이미지)로 이미 학습된 모델을 출발점으로 사용
- 이 모델들은 일반적인 언어 이해, 이미지 특징 추출 등 범용 능력을 이미 보유

**가중치 미세 조정**
- 사전 학습된 모델의 가중치를 초기값으로 설정
- 특정 작업에 맞는 소량의 라벨링 데이터로 추가 학습
- 낮은 학습률(learning rate)로 천천히 조정해 기존 지식은 유지하면서 특화 능력 추가

**전이학습의 한 형태**
- 한 도메인(대규모 범용 데이터)에서 학습한 지식을 다른 도메인(특정 작업)으로 전이(Transfer)
- 처음부터 학습(From Scratch)보다 훨씬 적은 데이터·시간·비용으로 높은 성능 달성

## 왜 파인튜닝을 사용하나?

**시간·비용 절감**
- 대형 모델을 처음부터 학습하려면 수천 GPU·수주~수개월 소요
- 파인튜닝은 수 시간~수일로 단축

**적은 데이터로 높은 성능**
- 사전 학습 모델이 이미 일반 지식 보유
- 수백~수천 개의 라벨링 데이터만으로도 특화 작업에서 우수한 성능

**범용 지식 유지**
- 사전 학습으로 얻은 언어 이해, 이미지 패턴 인식 등 기본 능력은 그대로 유지
- 특정 도메인 지식만 추가

**빠른 프로토타입 및 배포**
- 비즈니스 요구사항에 빠르게 대응
- 실험·검증·배포 사이클 단축

## 파인튜닝 방식

### 1. Full Fine-tuning (전체 파인튜닝)
- 모델의 모든 레이어·파라미터를 업데이트
- 가장 높은 성능, 하지만 많은 연산·메모리 필요
- 데이터가 충분하고 도메인 특화가 중요할 때 사용

### 2. Layer Freezing (레이어 동결)
- 하위 레이어(일반적 특징 추출)는 고정(Freeze)
- 상위 레이어(태스크 특화)만 학습
- 빠르고 메모리 효율적, 성능은 Full보다 약간 낮을 수 있음

### 3. Parameter-Efficient Fine-tuning (효율적 파인튜닝)
- LoRA(Low-Rank Adaptation): 작은 어댑터 모듈만 추가해 학습
- Adapter Tuning: 기존 레이어 사이에 작은 어댑터 삽입
- Prefix Tuning: 입력에 학습 가능한 prefix 추가
- 원본 모델 가중치는 그대로, 추가 파라미터만 학습 → 메모리·속도 최적화

## 파인튜닝 vs 기타 접근법

| 방식 | 설명 | 장점 | 단점 |
|------|------|------|------|
| From Scratch | 처음부터 학습 | 완전 커스텀 | 대량 데이터·시간·비용 필요 |
| 파인튜닝 | 사전 학습 모델 조정 | 빠름, 적은 데이터, 높은 성능 | 사전 학습 모델에 의존 |
| 프롬프트 엔지니어링 | 입력 프롬프트 설계 | 모델 수정 불필요, 즉시 적용 | 일관성·정확도 제한적 |
| RAG | 외부 지식 검색 후 생성 | 최신 정보 반영, 환각 감소 | 검색 시스템 필요 |

## 주요 활용 사례

**자연어 처리(NLP)**
- 감성 분석: 영화 리뷰, 제품 리뷰 긍정/부정 분류
- 도메인 특화 챗봇: 금융, 의료, 법률 상담 봇
- 개체명 인식(NER): 특정 산업의 고유명사 추출
- 문서 분류: 사내 문서, 이메일, 뉴스 자동 분류

**컴퓨터 비전(CV)**
- 이미지 분류: 제품 결함 검출, 의료 영상 진단
- 객체 탐지: 특정 산업용 제품·장비 인식
- 스타일 전이: 특정 화풍, 브랜드 스타일 학습

**생성형 AI**
- LLM 파인튜닝: GPT, LLaMA 등을 조직 데이터로 학습
- 코드 생성: 특정 프레임워크·코딩 스타일 맞춤
- 이미지 생성: Stable Diffusion을 특정 스타일로 조정

**음성 인식·합성**
- 특정 억양, 전문 용어 인식
- 브랜드 전용 음성 합성

## 파인튜닝 프로세스

### 1. 사전 학습 모델 선택
- HuggingFace, OpenAI, AWS SageMaker 등에서 모델 선택
- 작업과 유사한 도메인으로 사전 학습된 모델 우선

### 2. 데이터 준비
- 라벨링된 데이터 수집(분류, QA, 생성 등)
- 데이터 품질 검증, 전처리, 증강(Data Augmentation)
- 학습/검증/테스트 세트 분리

### 3. 하이퍼파라미터 설정
- 학습률(Learning Rate): 사전 학습보다 낮게(2e-5 ~ 5e-5)
- 배치 크기, Epoch 수, Warmup 단계 등

### 4. 학습 실행
- GPU/TPU에서 모델 학습
- 검증 세트로 성능 모니터링, 오버피팅 방지

### 5. 평가 및 최적화
- 테스트 세트로 최종 성능 평가
- 하이퍼파라미터 튜닝, 데이터 증강 등으로 반복 개선

### 6. 배포
- 최종 모델을 API, 애플리케이션에 통합
- 모니터링 및 지속적 개선

## 파인튜닝 시 주의사항

**과적합(Overfitting) 방지**
- 데이터가 적으면 모델이 학습 데이터만 암기
- Dropout, 데이터 증강, Early Stopping 활용

**Catastrophic Forgetting (파국적 망각)**
- 파인튜닝 중 사전 학습 지식을 잃어버림
- 낮은 학습률, Regularization으로 완화

**데이터 품질**
- 라벨링 오류, 편향된 데이터는 모델 성능 저하
- 데이터 품질 검증 필수

**리소스 관리**
- 대형 모델(GPT-3, LLaMA-70B)은 파인튜닝에도 많은 GPU 필요
- LoRA, QLoRA 등 효율적 기법 활용

**윤리·보안**
- 사전 학습 모델의 편향(Bias) 상속 가능
- 민감 데이터로 학습 시 정보 유출 위험
- 필터링, 감사, 접근 제어 필수

## 주요 도구 및 프레임워크

**오픈소스**
- HuggingFace Transformers: 다양한 사전 학습 모델 및 파인튜닝 스크립트 제공
- PyTorch, TensorFlow: 딥러닝 프레임워크
- FastAI: 간단한 파인튜닝 API

**클라우드 서비스**
- AWS SageMaker: 관리형 학습·배포
- Azure OpenAI, Google Vertex AI: LLM 파인튜닝 서비스
- OpenAI Fine-tuning API: GPT 모델 직접 파인튜닝

**효율화 라이브러리**
- LoRA, QLoRA(bitsandbytes): 메모리 효율적 파인튜닝
- PEFT(Parameter-Efficient Fine-Tuning): HuggingFace 공식 라이브러리

## 요약

파인튜닝은 **사전 학습된 모델의 범용 지식을 유지하면서, 특정 작업·도메인에 맞게 가중치를 미세 조정하는 효율적인 머신러닝 학습 방법**이다. 처음부터 학습하는 것보다 훨씬 적은 시간·비용·데이터로 높은 성능을 달성할 수 있으며, 현대 AI 개발의 핵심 실천 방법이다.
