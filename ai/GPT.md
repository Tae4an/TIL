# GPT란?

## 개념 정의
GPT(Generative Pre-trained Transformer)는 **대규모 텍스트 데이터로 사전 학습되어 자연어를 생성할 수 있는 트랜스포머 기반 대형 언어 모델(LLM)** 이다. OpenAI가 개발했으며, 생성형(Generative), 사전 학습(Pre-trained), 트랜스포머(Transformer)의 세 가지 핵심 특성을 담고 있다.

쉽게 말해 "방대한 텍스트로 미리 학습되어, 이전 단어들을 보고 다음 단어를 예측하며 사람처럼 자연스러운 문장을 생성하는 AI 언어 모델"이다.

## 핵심 특징

**생성형(Generative)**
- 주어진 프롬프트를 바탕으로 새로운 텍스트를 생성
- 질문에 답하기, 글쓰기, 코드 작성, 번역, 요약 등 다양한 생성 작업 수행
- 단순 분류·검색이 아닌 창의적 콘텐츠 생성 가능

**사전 학습(Pre-trained)**
- 라벨 없는 대규모 텍스트 데이터(수십억~수조 개 단어)로 비지도 학습
- 언어 구조, 문법, 상식, 추론 능력을 사전에 학습
- 특정 작업에 맞게 미세 조정(Fine-tuning) 또는 프롬프트만으로 활용 가능

**트랜스포머 아키텍처**
- Self-Attention 메커니즘으로 문맥 내 모든 단어 간 관계를 병렬 처리
- 긴 문맥 의존성을 효과적으로 학습
- RNN/LSTM 없이도 장거리 문맥 이해 가능

**자기회귀(Autoregressive) 생성**
- 이전 모든 단어를 보고 다음 단어를 예측하는 방식
- 왼쪽→오른쪽 단방향(Unidirectional) 문맥 처리
- 각 토큰의 확률 분포를 계산해 가장 적합한 단어 선택

## GPT 아키텍처

### 트랜스포머 디코더 기반
- GPT는 Transformer의 **디코더(Decoder) 부분만** 사용
- BERT는 인코더, GPT는 디코더 → 생성 작업에 특화

### 구조
```
입력 토큰
↓
Token Embedding + Position Embedding
↓
Transformer Decoder Block 1
  - Masked Multi-Head Self-Attention (미래 단어 가림)
  - Add & Norm
  - Feed Forward Network
  - Add & Norm
↓
... (여러 층 반복)
↓
Transformer Decoder Block N
↓
Output Layer (다음 토큰 확률 분포)
```

### Masked Self-Attention
- 현재 위치의 단어는 이전 단어들만 참조 가능(미래 단어는 가림)
- 자기회귀 생성을 위한 핵심 메커니즘
- BERT의 양방향 Attention과 차이

## GPT 발전 과정

### GPT-1 (2018)
- 파라미터: 117M (1억 1천7백만)
- 트랜스포머 디코더 12층
- 사전 학습 + 미세 조정 패러다임 확립
- 다양한 NLP 작업에서 좋은 성능

### GPT-2 (2019)
- 파라미터: 1.5B (15억)
- 더 큰 모델, 더 많은 데이터
- Zero-shot 학습 능력 입증: 미세 조정 없이도 작업 수행
- 처음에는 "너무 위험하다"며 공개 보류 후 나중에 오픈소스화

### GPT-3 (2020)
- 파라미터: 175B (1750억)
- Few-shot 학습: 몇 개 예시만으로 새 작업 수행
- In-context Learning: 프롬프트만으로 다양한 작업 처리
- API 형태로 제공 (OpenAI API)

### GPT-3.5 (2022)
- ChatGPT의 기반 모델
- Instruction Tuning, RLHF(인간 피드백 강화학습)으로 대화 능력 강화
- 사용자 친화적 대화형 인터페이스

### GPT-4 (2023)
- 멀티모달(텍스트 + 이미지 입력 가능)
- 더 긴 컨텍스트 길이(32K 토큰)
- 추론·창의성·안전성 대폭 향상
- 전문 시험(변호사, 의사, 회계사 등)에서 인간 수준 성능

### GPT-4o, GPT-4 Turbo 등
- 속도·비용 최적화
- 실시간 음성 대화 지원
- 더 긴 컨텍스트, 더 최신 지식

## GPT vs BERT 비교

| 특징 | GPT | BERT |
|------|-----|------|
| 아키텍처 | Transformer 디코더 | Transformer 인코더 |
| 학습 방향 | 단방향(좌→우) | 양방향(좌↔우) |
| 사전 학습 방식 | 다음 단어 예측(Autoregressive) | Masked Language Model + NSP |
| 강점 | 텍스트 생성, 대화, 창작 | 문맥 이해, 분류, 질의응답 |
| 대표 활용 | ChatGPT, 코드 생성, 글쓰기 | 검색, 분류, 개체명 인식 |

## 주요 활용 사례

**대화형 AI (챗봇)**
- ChatGPT, Bing Chat, Bard 등
- 고객 지원, 상담, 교육용 봇

**콘텐츠 생성**
- 블로그, 기사, 마케팅 카피 작성
- 창의적 글쓰기(소설, 시, 대본)

**코드 생성 및 프로그래밍 지원**
- GitHub Copilot (GPT 기반)
- 코드 자동 완성, 디버깅, 리팩토링

**번역 및 요약**
- 자연스러운 번역
- 긴 문서를 핵심만 요약

**교육 및 학습 지원**
- 과외 선생님처럼 개념 설명
- 문제 풀이, 에세이 피드백

**데이터 분석 및 인사이트 추출**
- 자연어로 데이터 질의
- 복잡한 리포트 자동 생성

**게임·엔터테인먼트**
- 인터랙티브 스토리텔링
- NPC 대화 생성

## 학습 방식

### 1. 사전 학습(Pre-training)
- 대규모 텍스트 코퍼스(웹 페이지, 책, 위키백과 등)로 학습
- 목표: 이전 단어들로 다음 단어 예측
- 비지도 학습(레이블 불필요)

### 2. 미세 조정(Fine-tuning) - GPT-1/2 시대
- 특정 작업(분류, QA 등)에 맞게 레이블 데이터로 추가 학습
- GPT-3 이후는 주로 프롬프트만으로 사용

### 3. Instruction Tuning (GPT-3.5+)
- 명령어-응답 쌍 데이터로 학습
- "~해줘", "~설명해줘" 같은 지시를 따르도록 학습

### 4. RLHF (인간 피드백 강화학습)
- 사람이 평가한 응답 품질로 모델 최적화
- 더 유용하고, 정직하고, 무해한(Helpful, Honest, Harmless) 응답 생성

## Few-shot & Zero-shot 학습

**Zero-shot Learning**
- 예시 없이 프롬프트만으로 작업 수행
- 예: "다음 문장을 프랑스어로 번역해줘: Hello"

**Few-shot Learning**
- 몇 개 예시를 프롬프트에 포함해 작업 수행
```
영어 → 한국어 번역:
Hello → 안녕하세요
Thank you → 감사합니다
Good morning → [GPT가 '좋은 아침'이라고 예측]
```

**In-context Learning**
- 모델 가중치 수정 없이 프롬프트만으로 학습
- GPT-3의 핵심 능력

## 한계 및 도전 과제

**환각(Hallucination)**
- 사실이 아닌 내용을 그럴듯하게 생성
- 최신 정보, 전문 지식에서 오류 가능

**지식 컷오프(Knowledge Cutoff)**
- 학습 시점 이후 정보는 모름
- GPT-4는 2023년 4월까지 데이터 학습

**편향(Bias)**
- 학습 데이터의 편향이 모델에 반영
- 성별, 인종, 문화적 편견 가능

**긴 문맥 처리 한계**
- 토큰 제한(GPT-3: 4K, GPT-4: 32K~128K)
- 매우 긴 문서는 처리 어려움

**추론 능력 한계**
- 복잡한 수학, 논리 추론에서 실수 가능
- 단순 패턴 매칭에 의존하는 경향

**비용**
- 대형 모델 학습·운영 비용 높음
- API 호출 비용(토큰당 과금)

## 실무 활용 팁

**효과적인 프롬프트 작성**
- 명확하고 구체적인 지시
- 예시 제공(Few-shot)
- 역할 부여("당신은 전문 번역가입니다")

**환각 방지**
- RAG(검색 증강 생성)로 외부 지식 활용
- "모르면 모른다고 해"라고 명시
- 중요 정보는 검증 필수

**비용 최적화**
- 짧은 프롬프트, 적절한 모델 선택
- 캐싱으로 중복 호출 방지
- 배치 처리

**안전 및 윤리**
- 콘텐츠 필터링, 사용자 입력 검증
- 개인정보 보호, 저작권 존중
- 편향·차별 방지 가이드라인

## 관련 도구 및 플랫폼

**OpenAI API**
- GPT-3.5, GPT-4 API 제공
- ChatGPT, DALL-E, Whisper 통합

**Azure OpenAI Service**
- MS Azure에서 제공하는 엔터프라이즈용 GPT 서비스

**AWS Bedrock**
- GPT 외 다양한 LLM 제공

**오픈소스 대안**
- GPT-Neo, GPT-J (EleutherAI)
- LLaMA (Meta), Falcon, Mistral 등

## 요약

GPT는 트랜스포머 디코더 기반의 자기회귀 언어 모델로, 대규모 사전 학습을 통해 자연어 생성·이해 능력을 갖춘 현대 생성형 AI의 대표 주자다. ChatGPT로 대중화되었으며, 대화·글쓰기·코드 생성·창의적 작업 등 광범위한 분야에서 활용되고 있다. BERT가 이해(인코더)에 강하다면, GPT는 생성(디코더)에 강한 모델이라 할 수 있다.
