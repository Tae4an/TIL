# BLEU란?

## 개념 정의
BLEU(Bilingual Evaluation Understudy)는 **기계 번역 결과를 사람이 번역한 참조 텍스트와 비교해 번역 품질을 자동으로 평가하는 정량적 지표**다. 2002년 IBM 연구진이 개발했으며, 기계 번역(MT) 시스템 평가의 사실상 표준 지표가 되었다.

쉽게 말해 "AI 번역이 사람 번역과 얼마나 비슷한지 n-gram 겹침 정도로 점수를 매기는 자동 평가 방법"이다.

## 핵심 원리

**n-gram 기반 정밀도(Precision)**
- n-gram: 연속된 n개의 단어 조합(1-gram=단어, 2-gram=단어쌍, 3-gram, 4-gram 등)
- 기계 번역 결과에 나온 n-gram이 참조 번역에도 있는지 확인
- 일치하는 n-gram 비율이 높을수록 높은 점수

**Modified Precision (클리핑)**
- 같은 단어가 반복될 경우 과대평가 방지
- 참조 문장에서 등장한 최대 횟수까지만 카운트
- 예: 기계 번역 "the the the" vs 참조 "the cat" → "the"는 1회만 인정

**Brevity Penalty (BP, 길이 페널티)**
- 기계 번역이 너무 짧으면 정밀도만으로는 과대평가될 수 있음
- 출력 문장이 참조보다 짧으면 페널티 부과
- 적절한 길이 유도

**다양한 n-gram 결합**
- 보통 1-gram ~ 4-gram까지 정밀도를 각각 계산
- 기하평균(Geometric Mean)으로 결합
- 단어 일치(1-gram) + 구문 일치(2~4-gram) 모두 고려

## BLEU 점수 계산 공식

$$
BLEU = BP \times \left( \prod_{n=1}^{N} p_n \right)^{\frac{1}{N}}
$$

여기서:
- \( p_n \): n-gram 정밀도 (보통 N=4)
- \( BP \): Brevity Penalty

$$
BP = \begin{cases}
1 & \text{if } c > r \\
e^{(1-r/c)} & \text{if } c \leq r
\end{cases}
$$

- \( c \): 기계 번역 길이 (candidate length)
- \( r \): 참조 번역 길이 (reference length)

## 점수 범위 및 해석

**점수 범위: 0.0 ~ 1.0 (또는 0 ~ 100으로 표기)**
- 1.0 (100): 완벽히 일치 (실제로는 거의 불가능)
- 0.0 (0): 전혀 일치하지 않음

**일반적인 해석 기준**
- BLEU > 0.5 (50): 이해 가능하고 품질 좋은 번역
- 0.3 ~ 0.5 (30~50): 대체로 이해 가능한 번역
- 0.2 ~ 0.3 (20~30): 부분적으로 이해 가능
- < 0.2 (20): 품질 낮음

**언어쌍·도메인별 차이**
- 영어-프랑스어: 상대적으로 높은 점수 (언어 구조 유사)
- 영어-한국어/일본어: 낮은 점수 경향 (어순·구조 차이)
- 전문 도메인(의료, 법률): 일반 도메인보다 낮은 점수

## 계산 예시

**참조 번역(Reference):**
"The cat is on the mat"

**기계 번역(Candidate):**
"The cat the mat"

**1-gram 정밀도:**
- 기계 번역 n-gram: the(2), cat(1), mat(1) → 총 4개
- 참조 일치: the(1 클립), cat(1), mat(1) → 3개 일치
- p1 = 3/4 = 0.75

**2-gram 정밀도:**
- 기계 번역: "the cat", "cat the", "the mat"
- 참조 일치: "the cat"(1), "the mat"(1) → 2개
- p2 = 2/3 = 0.67

**Brevity Penalty:**
- c=4 (기계 번역 길이), r=6 (참조 길이)
- BP = e^(1-6/4) = e^(-0.5) ≈ 0.61

**BLEU 점수:**
- BLEU = 0.61 × (0.75 × 0.67)^0.5 ≈ 0.43

## 주요 활용 사례

**기계 번역 시스템 평가**
- Google Translate, DeepL, Papago 등 번역 품질 비교
- 번역 모델 개발 시 성능 추적

**모델 개발·튜닝**
- 학습 중 BLEU로 검증 세트 평가
- 하이퍼파라미터 조정, 모델 선택

**A/B 테스트**
- 새 모델 vs 기존 모델 성능 비교
- 빠른 자동 평가로 실험 가속

**번역 시스템 모니터링**
- 프로덕션 환경에서 번역 품질 추적
- 품질 저하 조기 감지

**다국어 NLP 작업**
- 텍스트 요약, 질의응답, 이미지 캡셔닝 등에도 응용
- 생성된 텍스트와 참조 텍스트 비교

## BLEU의 장점

**빠른 자동 평가**
- 사람 평가 없이 즉시 점수 계산
- 대규모 평가·실험에 효율적

**언어 독립적**
- 어떤 언어쌍에도 적용 가능
- 별도 언어별 규칙 불필요

**재현 가능**
- 같은 데이터·참조에 대해 일관된 점수
- 실험 비교·재현이 용이

**표준화**
- 학계·산업계에서 널리 사용
- 논문·벤치마크에서 표준 지표

## BLEU의 한계 및 단점

**의미·문맥 이해 부족**
- 단어 겹침만 보고, 의미가 같아도 단어가 다르면 낮은 점수
- 예: "big" vs "large", "happy" vs "joyful" → 동의어인데 불일치로 판정

**문법·유창성 미반영**
- 문법적으로 틀려도 단어만 맞으면 높은 점수 가능
- 자연스러움, 가독성 평가 불가

**참조 번역 의존성**
- 참조 번역이 하나뿐이면 다양한 올바른 표현 반영 못함
- 참조가 여러 개 있을수록 신뢰도 향상

**짧은 문장 편향**
- 문장이 짧을수록 높은 점수 경향 (BP로 어느 정도 완화)
- 긴 문장에서는 BLEU 점수 하락

**절대 점수 비교 어려움**
- 언어쌍·도메인·데이터셋에 따라 점수 범위 상이
- 영어-프랑스어 40점과 영어-중국어 40점은 다른 의미

## 개선 지표 및 대안

**BLEU 변형**
- SacreBLEU: 표준화된 BLEU 구현, 재현성 향상
- sentBLEU: 문장 레벨 BLEU
- chrF: 문자 n-gram 기반 (형태 변화 많은 언어에 유리)

**의미 기반 지표**
- METEOR: 동의어, 형태 변화 고려
- BERTScore: BERT 임베딩 기반 의미 유사도
- COMET: 신경망 기반 평가 모델 (사람 판단과 높은 상관)

**Task별 특화 지표**
- ROUGE: 요약 평가
- CIDEr: 이미지 캡셔닝 평가
- Human Evaluation: 최종적으로 사람 평가 필수

## 실무 사용 팁

**여러 참조 번역 사용**
- 가능하면 참조를 3~5개 준비
- 다양한 올바른 표현 반영

**도메인·언어쌍 고려**
- 절대 점수보다는 같은 조건에서 상대 비교
- 언어쌍별 기준 점수 미리 파악

**다른 지표와 병행**
- BLEU만으로 판단하지 말고 METEOR, BERTScore 등 함께 사용
- 최종 검증은 사람 평가

**Python 구현**
```python
from nltk.translate.bleu_score import sentence_bleu, corpus_bleu

reference = [['the', 'cat', 'is', 'on', 'the', 'mat']]
candidate = ['the', 'cat', 'the', 'mat']

score = sentence_bleu(reference, candidate)
print(f"BLEU score: {score:.4f}")
```

## 요약

BLEU는 기계 번역 품질을 n-gram 겹침으로 평가하는 빠르고 표준화된 자동 지표로, 개발·실험·모니터링에 필수적이다. 하지만 의미·문법·유창성 평가 한계가 있어, 다른 지표 및 사람 평가와 함께 사용해야 종합적인 번역 품질을 파악할 수 있다.
