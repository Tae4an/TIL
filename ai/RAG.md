# RAG란?
RAG는 **검색 증강 생성(Retrieval-Augmented Generation)** 의 약자로, LLM 같은 생성형 AI에 **외부 검색 시스템을 붙여서 더 정확하고 최신·도메인 특화된 답을 만들게 하는 아키텍처**다.

## 개념과 목적
- LLM은 학습 시점 이후 정보나 사내 비공개 문서를 “모른다”는 한계가 있고, 할루시네이션(그럴듯한 거짓말)도 잦다.
- RAG는 **질문이 들어올 때마다** 벡터DB·문서 저장소·DB 등에서 관련 문서를 먼저 검색(Retrieval)하고, 이 텍스트를 LLM 프롬프트에 같이 넣어(컨텍스트로 주입) 답변을 생성(Generation)하게 만든다.
- 이렇게 하면 모델을 재학습/파인튜닝하지 않고도 **최신성, 정확성, 도메인 특화성**을 크게 끌어올릴 수 있다.

## 동작 흐름

1. **질문 입력**
   - 사용자가 질문을 던진다 (예: “우리 회사 VPN 정책 요약해줘”).

2. **임베딩 & 검색(Retrieval)**
   - 질문을 임베딩(벡터화)하고, 벡터 데이터베이스/검색엔진에서 의미적으로 유사한 문서들을 k개 정도 가져온다.
   - 소스: 사내 위키, PDF, 노션, DB 레코드, 로그, 매뉴얼 등.

3. **프롬프트 구성**
   - “질문 + 검색된 문서들”을 하나의 큰 컨텍스트로 묶어 LLM에 넣는다.

4. **생성(Generation)**
   - LLM이 제공된 컨텍스트를 바탕으로 답변을 생성한다.  
   - 이때 모델은 내부 파라미터 지식 + 검색된 외부 지식을 함께 활용한다.

5. **(선택) 후처리**
   - 출처 링크 달기, 인용 리스트 만들기, 추가 Rerank/필터링 등.

## RAG의 장점

- **최신 정보 반영**: 학습 이후 생긴 문서·정책·뉴스도 검색을 통해 바로 반영.
- **도메인 특화**: 사내 문서, 전문 도메인(의료·법률·클라우드 아키텍처 등)에 대해 정확한 답변 제공.
- **할루시네이션 감소**: 모델이 “모르는 걸 지어내기”보다, 실제 문서를 근거로 답하게 만들 수 있어 신뢰성 향상.
- **모델 재학습 불필요**: LLM 파인튜닝 없이도 지식 확장이 가능해 비용·시간 절약.

## 파인튜닝과의 차이

- **파인튜닝**:  
  - 모델 **가중치 자체를 수정**해서 특정 태스크/도메인에 맞게 “뇌 구조”를 바꾸는 방식.
  - 장점: 태스크 특화 패턴, 스타일, 추론 방식 내재화.  
  - 단점: 비용 크고, 재학습 필요, 최신 데이터 반영이 느림.

- **RAG**:  
  - 모델은 그대로 두고, **프롬프트에 “외부 지식”을 주입**해서 성능을 끌어올리는 방식.
  - 장점: 빠른 지식 업데이트, 사내 비공개 데이터 활용, 재학습 불필요.  
  - 단점: 검색 품질·임베딩·벡터DB 설계가 중요, 컨텍스트 길이 한계 존재.

현업에서 보통은 **RAG를 먼저 도입해서 “지식·문서 문제”를 해결**하고, 그래도 부족한 “스타일/추론/태스크 특화” 영역이 남을 때 파인튜닝을 추가로 고려하는 패턴이 많다.
