# 데이터 레이크란?

## 개념 정의
데이터 레이크(Data Lake)는 정형·반정형·비정형 데이터를 포함한 모든 규모와 형식의 원시(raw) 데이터를 있는 그대로 저장할 수 있는 중앙 집중식 저장소다. 데이터를 미리 구조화하거나 가공하지 않고 네이티브 형식으로 저장하며, 필요할 때 다양한 분석·머신러닝·비즈니스 인텔리전스 용도로 유연하게 활용할 수 있다.

쉽게 말해 "모든 종류의 데이터를 그대로 쏟아 넣을 수 있는 거대한 저장 호수로, 나중에 필요할 때 원하는 방식으로 데이터를 꺼내 쓸 수 있는 유연한 빅데이터 저장소"다.

## 핵심 특징

**원시 데이터 저장**
- 데이터를 원본 형식 그대로 저장
- 사전 변환·정제·구조화 불필요
- 로그, 센서 데이터, 이미지, 동영상, JSON, CSV 모두 저장 가능

**스키마 온 리드(Schema-on-Read)**
- 데이터 저장 시점에는 스키마 정의 안 함
- 데이터 읽을 때(분석 시점) 스키마 적용
- 유연성 극대화 (다양한 목적으로 재사용 가능)

**확장성**
- 테라바이트(TB)에서 페타바이트(PB) 규모까지 확장
- 클라우드 스토리지(S3, Azure Data Lake Storage) 활용
- 수평 확장 용이

**비용 효율성**
- 저렴한 객체 스토리지 활용
- 데이터 웨어하우스 대비 저장 비용 낮음
- 필요한 데이터만 처리해 컴퓨팅 비용 절감

**다양한 데이터 유형 지원**
- 정형 데이터: 데이터베이스 테이블, CSV, Excel
- 반정형 데이터: JSON, XML, 로그 파일
- 비정형 데이터: 이미지, 동영상, 오디오, 텍스트 문서, 소셜 미디어

**다목적 활용**
- 빅데이터 분석
- 머신러닝·AI 모델 훈련
- 실시간 스트리밍 분석
- 데이터 탐색·발견
- 백업·아카이브

## 데이터 레이크 아키텍처

### 3단계 구조

**1. 랜딩 존(Landing Zone) - 원시 데이터 영역**
- 다양한 소스에서 수집된 데이터를 그대로 적재
- 변환·정제 전 상태
- 원본 데이터 보존 (불변성)
- 예: `/raw/logs/2026/01/07/`, `/raw/iot-sensors/`

**2. 스테이징 존(Staging Zone) - 정제·변환 영역**
- 원시 데이터를 분석 가능한 형태로 변환
- 데이터 정제, 중복 제거, 포맷 변환
- ETL/ELT 프로세스 수행
- 예: `/processed/cleaned-logs/`, `/processed/aggregated-sales/`

**3. 데이터 탐색 존(Data Exploration Zone) - 큐레이션된 데이터**
- 분석·머신러닝·BI 도구가 사용하는 최종 데이터
- 품질 검증된 데이터
- 최적화된 형식(Parquet, ORC, Delta Lake)
- 예: `/curated/customer-360/`, `/curated/ml-features/`

### 주요 구성 요소

**스토리지 계층**
- 클라우드 객체 스토리지: AWS S3, Azure Data Lake Storage, Google Cloud Storage
- 분산 파일 시스템: HDFS(Hadoop Distributed File System)

**데이터 수집(Ingestion)**
- 배치 수집: Apache Sqoop, AWS Glue, Azure Data Factory
- 스트리밍 수집: Apache Kafka, AWS Kinesis, Azure Event Hubs
- 실시간 수집: IoT Hub, Flume

**데이터 카탈로그 & 메타데이터 관리**
- AWS Glue Data Catalog
- Azure Purview
- Apache Atlas
- 데이터 검색, 스키마 추론, 계보(Lineage) 추적

**데이터 처리 & 변환**
- 배치 처리: Apache Spark, AWS EMR, Azure Databricks
- 스트림 처리: Apache Flink, Kafka Streams
- ETL 도구: AWS Glue, Azure Synapse Analytics

**데이터 보안 & 거버넌스**
- 접근 제어: IAM, RBAC, ABAC
- 암호화: 저장 시·전송 중 암호화
- 감사: CloudTrail, Azure Monitor
- 데이터 품질·계보 관리

**분석 & 쿼리**
- SQL 쿼리 엔진: AWS Athena, Azure Synapse Serverless, Presto, Trino
- 머신러닝: AWS SageMaker, Azure ML, TensorFlow, PyTorch
- BI 도구: Tableau, Power BI, QuickSight

## 데이터 레이크 vs 데이터 웨어하우스

| 항목 | 데이터 레이크 | 데이터 웨어하우스 |
|------|-------------|----------------|
| 데이터 유형 | 정형, 반정형, 비정형 모두 | 정형 데이터만 |
| 스키마 | Schema-on-Read (읽을 때 정의) | Schema-on-Write (쓸 때 정의) |
| 저장 형식 | 원시 데이터 (네이티브 형식) | 정제·변환된 데이터 |
| 유연성 | 매우 높음 (목적 미정의 가능) | 낮음 (특정 BI 용도) |
| 사용자 | 데이터 과학자, 엔지니어 | 비즈니스 분석가, 경영진 |
| 처리 방식 | ELT (Extract-Load-Transform) | ETL (Extract-Transform-Load) |
| 쿼리 성능 | 느림 (데이터 규모·복잡도에 따라) | 빠름 (사전 최적화) |
| 비용 | 저장 비용 낮음 | 저장 비용 높음 |
| 확장성 | 무한 확장 가능 | 제한적 |
| 주요 용도 | 머신러닝, 빅데이터 분석, 탐색 | 보고서, 대시보드, OLAP |
| 데이터 신선도 | 실시간 가능 | 배치 업데이트 (일반적) |

### 예시 비교

**데이터 레이크 사용 사례**
- "모든 고객 클릭스트림 로그를 저장해서 나중에 ML 모델 훈련에 사용"
- "IoT 센서 데이터를 실시간 수집해 이상 탐지"
- "소셜 미디어 텍스트·이미지를 수집해 감성 분석"

**데이터 웨어하우스 사용 사례**
- "분기별 매출 보고서 생성"
- "지역별 판매 실적 대시보드"
- "고객 세그먼트별 매출 분석"

## 데이터 레이크하우스(Data Lakehouse)

### 개념
데이터 레이크와 데이터 웨어하우스의 장점을 결합한 하이브리드 아키텍처로, 저렴한 객체 스토리지 위에 트랜잭션·스키마·거버넌스 기능을 추가한다.

### 주요 기술

**Delta Lake (Databricks)**
- ACID 트랜잭션 지원
- 타임 트래블(Time Travel) - 과거 버전 조회
- 스키마 적용(Schema Enforcement)
- Upsert/Delete/Merge 연산

**Apache Iceberg (Netflix)**
- 테이블 포맷 표준화
- 숨겨진 파티셔닝
- 스냅샷 격리

**Apache Hudi (Uber)**
- 증분 데이터 처리
- 레코드 레벨 업데이트

### 장점
- 데이터 레이크의 유연성 + 데이터 웨어하우스의 성능
- 단일 플랫폼에서 BI와 AI 지원
- 비용 효율적

## 주요 활용 사례

### 1. 빅데이터 분석

**시나리오**
- 페타바이트 규모의 웹 로그, 트랜잭션 데이터 분석
- 고객 행동 패턴 발견

**방법**
- 원시 로그를 S3에 저장
- Spark로 배치 처리
- Athena로 SQL 쿼리
- QuickSight로 시각화

### 2. 머신러닝 & AI

**시나리오**
- 추천 시스템, 이상 탐지, 이미지 분류 모델 훈련

**방법**
- 다양한 소스에서 데이터 수집 (로그, 이미지, 텍스트)
- 데이터 레이크에 저장
- SageMaker, TensorFlow로 모델 훈련
- 피처 스토어로 특성 관리

### 3. 실시간 스트리밍 분석

**시나리오**
- IoT 센서 데이터 실시간 모니터링
- 사기 탐지, 이상 징후 실시간 알람

**방법**
- Kafka, Kinesis로 스트리밍 데이터 수집
- Flink, Spark Streaming으로 실시간 처리
- 결과를 데이터 레이크 저장
- Lambda로 알람 전송

### 4. 고객 360도 뷰

**시나리오**
- 웹, 모바일, 오프라인, 콜센터 등 모든 고객 접점 데이터 통합

**방법**
- 다양한 소스 데이터를 데이터 레이크로 수집
- ETL로 통합·정제
- 단일 고객 프로필 생성
- CRM, 마케팅 자동화 도구에 제공

### 5. 규정 준수 & 아카이브

**시나리오**
- 법적 요구사항으로 7년간 로그 보관
- 감사 추적

**방법**
- 모든 거래·접근 로그를 데이터 레이크에 저장
- 저렴한 장기 보관
- 필요 시 검색·분석

### 6. 데이터 과학 실험

**시나리오**
- 탐색적 데이터 분석(EDA)
- 가설 검증, A/B 테스트

**방법**
- 원시 데이터를 자유롭게 탐색
- Jupyter Notebook, Zeppelin으로 실험
- 다양한 접근 방식 시도

## 데이터 레이크 구축 사례

### AWS 기반 데이터 레이크

**아키텍처**
```
데이터 소스 → Kinesis/Kafka → S3 (Data Lake)
                                 ↓
                         Glue ETL (변환)
                                 ↓
                  S3 Curated Zone (Parquet)
                                 ↓
                    Athena/Redshift Spectrum
                                 ↓
                        QuickSight (BI)
```

**주요 서비스**
- 스토리지: S3
- 수집: Kinesis Data Firehose, AWS DMS
- 카탈로그: Glue Data Catalog
- 처리: Glue ETL, EMR (Spark)
- 쿼리: Athena, Redshift Spectrum
- 보안: IAM, Lake Formation, KMS
- ML: SageMaker

### Azure 기반 데이터 레이크

**아키텍처**
```
데이터 소스 → Event Hubs → ADLS Gen2
                              ↓
                    Synapse Analytics (처리)
                              ↓
                        Databricks (ML)
                              ↓
                      Power BI (시각화)
```

**주요 서비스**
- 스토리지: Azure Data Lake Storage Gen2
- 수집: Event Hubs, Data Factory
- 카탈로그: Azure Purview
- 처리: Synapse Analytics, Databricks
- 보안: Azure AD, Private Link
- ML: Azure ML

## 데이터 레이크의 과제 (Data Swamp 위험)

### 1. 데이터 늪(Data Swamp) 문제

**원인**
- 무분별한 데이터 적재
- 메타데이터·카탈로그 부재
- 데이터 품질 관리 안 함
- 거버넌스 미비

**결과**
- 필요한 데이터를 찾을 수 없음
- 중복 데이터 급증
- 신뢰할 수 없는 데이터
- "쓰레기 저장소"로 전락

### 2. 보안 및 규정 준수

**과제**
- 민감한 데이터 노출 위험
- 접근 제어 복잡성
- GDPR, CCPA 등 규정 준수

**해결책**
- 데이터 분류·태깅
- 세밀한 접근 제어 (Row/Column Level)
- 암호화·마스킹
- 감사 로깅

### 3. 성능 문제

**과제**
- 원시 데이터 쿼리 시 느림
- 스캔 비용 증가 (전체 파일 읽기)

**해결책**
- 파티셔닝 (날짜, 지역 등)
- 컬럼형 포맷 (Parquet, ORC)
- 압축
- 인덱싱·캐싱

### 4. 데이터 품질

**과제**
- 중복·결측·오류 데이터
- 일관성 부족

**해결책**
- 데이터 품질 검증 자동화
- 스키마 검증
- 데이터 프로파일링

## 모범 사례

### 1. 데이터 거버넌스 수립

**명확한 정책**
- 데이터 보존 기간
- 접근 권한 정책
- 데이터 분류 체계

**데이터 카탈로그**
- 모든 데이터셋 메타데이터 관리
- 검색 가능하게 태깅
- 소유자·용도 명시

### 2. 존(Zone) 기반 구조화

**랜딩 → 스테이징 → 큐레이션 영역 분리**
- 원시 데이터는 변경 불가(Immutable)
- 각 단계별 명확한 역할
- 데이터 계보 추적 가능

### 3. 파티셔닝 & 최적화

**파티셔닝 전략**
- 날짜별: `/year=2026/month=01/day=07/`
- 지역별, 카테고리별 분할

**파일 포맷 최적화**
- Parquet, ORC (컬럼형 포맷)
- 압축 (Snappy, GZIP)

### 4. 보안 강화

**암호화**
- 저장 시: S3 SSE, ADLS 암호화
- 전송 중: TLS/SSL

**접근 제어**
- IAM, RBAC로 세밀한 권한 관리
- 최소 권한 원칙

**감사**
- CloudTrail, Azure Monitor로 모든 활동 로깅

### 5. 자동화

**데이터 수집 자동화**
- 스케줄된 배치 작업
- 실시간 스트리밍

**데이터 품질 검증 자동화**
- 스키마 검증
- 이상 탐지

### 6. 비용 최적화

**스토리지 계층화**
- S3 Intelligent-Tiering
- 오래된 데이터는 Glacier로 아카이브

**불필요한 데이터 삭제**
- 라이프사이클 정책
- 중복 제거

## 요약

데이터 레이크는 정형·반정형·비정형 데이터를 원시 형태로 저장하는 중앙 집중식 빅데이터 저장소로, Schema-on-Read 방식으로 유연성을 극대화하고 저렴한 객체 스토리지를 활용해 페타바이트 규모까지 확장 가능하다. 랜딩 존→스테이징 존→탐색 존의 3단계 아키텍처를 통해 원시 데이터를 수집·변환·큐레이션하며, 빅데이터 분석, 머신러닝, 실시간 스트리밍, 고객 360도 뷰, 규정 준수 등 다양한 용도로 활용된다. 데이터 웨어하우스는 정제된 정형 데이터로 빠른 BI를 제공하지만, 데이터 레이크는 모든 데이터를 보존해 탐색적 분석과 AI를 지원하며, 최근에는 두 장점을 결합한 데이터 레이크하우스(Delta Lake, Iceberg)가 주목받고 있다. 성공적 운영을 위해서는 데이터 거버넌스, 카탈로그, 파티셔닝, 보안, 자동화, 비용 최적화를 통해 데이터 늪(Data Swamp)을 방지해야 한다.
